{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-21T07:47:03.806330Z","iopub.status.busy":"2023-05-21T07:47:03.805788Z","iopub.status.idle":"2023-05-21T07:47:08.511824Z","shell.execute_reply":"2023-05-21T07:47:08.510892Z","shell.execute_reply.started":"2023-05-21T07:47:03.806290Z"},"trusted":true},"outputs":[],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.datasets import multi30k, Multi30k\n","from typing import Iterable, List\n","\n","\n","# We need to modify the URLs for the dataset since the links to the original dataset are broken\n","# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n","multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n","multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n","\n","SRC_LANGUAGE = 'de'\n","TGT_LANGUAGE = 'en'\n","\n","# Place-holders\n","token_transform = {}\n","vocab_transform = {}"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T07:47:08.514509Z","iopub.status.busy":"2023-05-21T07:47:08.513871Z","iopub.status.idle":"2023-05-21T07:47:48.600492Z","shell.execute_reply":"2023-05-21T07:47:48.599528Z","shell.execute_reply.started":"2023-05-21T07:47:08.514471Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting en-core-web-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 25.2 MB/s eta 0:00:00\n","Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.6)\n","Requirement already satisfied: jinja2 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: packaging>=20.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (5.2.1)\n","Requirement already satisfied: setuptools in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.7)\n","Requirement already satisfied: numpy>=1.15.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.24.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n","Requirement already satisfied: pathy>=0.10.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.6)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","Collecting de-core-news-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/14.6 MB 41.9 MB/s eta 0:00:00\n","Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from de-core-news-sm==3.5.0) (3.5.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.28.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: setuptools in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (65.6.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.6)\n","Requirement already satisfied: jinja2 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: pathy>=0.10.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.6)\n","Requirement already satisfied: packaging>=20.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (23.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.7)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.10)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (5.2.1)\n","Requirement already satisfied: numpy>=1.15.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.24.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.9.1)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.4.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.15)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2023.5.7)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/nanditraore/anaconda3/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n"]}],"source":["import spacy\n","import spacy.cli\n","spacy.cli.download(\"en_core_web_sm\")\n","spacy.cli.download(\"de_core_news_sm\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T07:47:48.602263Z","iopub.status.busy":"2023-05-21T07:47:48.601639Z","iopub.status.idle":"2023-05-21T07:47:48.608436Z","shell.execute_reply":"2023-05-21T07:47:48.607502Z","shell.execute_reply.started":"2023-05-21T07:47:48.602235Z"},"trusted":true},"outputs":[],"source":["import portalocker\n","from portalocker import *"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T07:47:48.612209Z","iopub.status.busy":"2023-05-21T07:47:48.611454Z","iopub.status.idle":"2023-05-21T07:47:58.088215Z","shell.execute_reply":"2023-05-21T07:47:58.087224Z","shell.execute_reply.started":"2023-05-21T07:47:48.612171Z"},"trusted":true},"outputs":[],"source":["token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n","token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n","\n","\n","\n","def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n","    \"\"\"\n","    Helper function that yields a list of tokens from the given data iterator and language.\n","\n","    Args:\n","        data_iter (Iterable): Iterable containing the data samples.\n","        language (str): Language code specifying the language of the data.\n","\n","    Returns:\n","        List[str]: List of tokens extracted from the data samples.\n","    \"\"\"\n","    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n","\n","    for data_sample in data_iter:\n","        yield token_transform[language](data_sample[language_index[language]])\n","\n","\n","# Define special symbols and indices\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n","\n","# Build vocabularies for source and target languages\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    # Training data Iterator\n","    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    # Create torchtext's Vocab object\n","    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n","                                                    min_freq=1,\n","                                                    specials=special_symbols,\n","                                                    special_first=True)\n","\n","\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    \"\"\"\n","    Set the default index of the vocabulary for a specific language.\n","\n","    Args:\n","        ln (str): Language code specifying the language of the vocabulary.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    vocab_transform[ln].set_default_index(UNK_IDX)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T07:47:58.089938Z","iopub.status.busy":"2023-05-21T07:47:58.089560Z","iopub.status.idle":"2023-05-21T07:47:58.109822Z","shell.execute_reply":"2023-05-21T07:47:58.108890Z","shell.execute_reply.started":"2023-05-21T07:47:58.089905Z"},"trusted":true},"outputs":[],"source":["from torch import Tensor\n","import torch\n","import torch.nn as nn\n","from torch.nn import Transformer\n","import math\n","\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    Module that adds positional encoding to the token embedding to introduce a notion of word order.\n","\n","    Args:\n","        emb_size (int): The embedding size of the tokens.\n","        dropout (float): The dropout probability.\n","        maxlen (int, optional): The maximum length of the sequence. Defaults to 5000.\n","    \"\"\"\n","    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n","        super(PositionalEncoding, self).__init__()\n","        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n","        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n","        pos_embedding = torch.zeros((maxlen, emb_size))\n","        pos_embedding[:, 0::2] = torch.sin(pos * den)\n","        pos_embedding[:, 1::2] = torch.cos(pos * den)\n","        pos_embedding = pos_embedding.unsqueeze(-2)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer('pos_embedding', pos_embedding)\n","\n","    def forward(self, token_embedding: Tensor) -> Tensor:\n","        \"\"\"\n","        Apply positional encoding to the token embeddings.\n","\n","        Args:\n","            token_embedding (Tensor): The token embeddings.\n","\n","        Returns:\n","            Tensor: The token embeddings with positional encoding applied.\n","        \"\"\"\n","        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n","\n","class TokenEmbedding(nn.Module):\n","    \"\"\"\n","    Module to convert tensor of input indices into corresponding tensor of token embeddings.\n","\n","    Args:\n","        vocab_size (int): The size of the vocabulary.\n","        emb_size (int): The embedding size.\n","    \"\"\"\n","    def __init__(self, vocab_size: int, emb_size: int):\n","        super(TokenEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, emb_size)\n","        self.emb_size = emb_size\n","\n","    def forward(self, tokens: Tensor) -> Tensor:\n","        \"\"\"\n","        Convert tensor of input indices into corresponding tensor of token embeddings.\n","\n","        Args:\n","            tokens (Tensor): The input indices.\n","\n","        Returns:\n","            Tensor: The token embeddings.\n","        \"\"\"\n","        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n","\n","class Seq2SeqTransformer(nn.Module):\n","    \"\"\"\n","    Seq2Seq Transformer model.\n","\n","    Args:\n","        num_encoder_layers (int): Number of encoder layers.\n","        num_decoder_layers (int): Number of decoder layers.\n","        emb_size (int): The embedding size.\n","        nhead (int): The number of attention heads.\n","        src_vocab_size (int): The size of the source vocabulary.\n","        tgt_vocab_size (int): The size of the target vocabulary.\n","        dim_feedforward (int, optional): The hidden size of the feedforward layers. Defaults to 512.\n","        dropout (float, optional): The dropout probability. Defaults to 0.1.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        num_encoder_layers: int,\n","        num_decoder_layers: int,\n","        emb_size: int,\n","        nhead: int,\n","        src_vocab_size: int,\n","        tgt_vocab_size: int,\n","        dim_feedforward: int = 512,\n","        dropout: float = 0.1,\n","    ):\n","        super(Seq2SeqTransformer, self).__init__()\n","        self.transformer = Transformer(\n","            d_model=emb_size,\n","            nhead=nhead,\n","            num"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T07:47:58.111424Z","iopub.status.busy":"2023-05-21T07:47:58.111023Z","iopub.status.idle":"2023-05-21T07:47:58.126575Z","shell.execute_reply":"2023-05-21T07:47:58.125628Z","shell.execute_reply.started":"2023-05-21T07:47:58.111387Z"},"trusted":true},"outputs":[],"source":["def generate_square_subsequent_mask(sz: int) -> Tensor:\n","    \"\"\"\n","    Generate a square subsequent mask of shape (sz, sz) for self-attention.\n","\n","    Args:\n","        sz (int): The size of the mask.\n","\n","    Returns:\n","        Tensor: The square subsequent mask.\n","    \"\"\"\n","    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n","    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","    return mask\n","\n","\n","def create_mask(src: Tensor, tgt: Tensor) -> tuple:\n","    \"\"\"\n","    Create masks for source and target sequences.\n","\n","    Args:\n","        src (Tensor): The source sequence.\n","        tgt (Tensor): The target sequence.\n","\n","    Returns:\n","        tuple: A tuple containing the source mask, target mask, source padding mask, and target padding mask.\n","    \"\"\"\n","    src_seq_len = src.shape[0]\n","    tgt_seq_len = tgt.shape[0]\n","\n","    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n","    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n","\n","    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n","    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n","    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T07:47:58.128180Z","iopub.status.busy":"2023-05-21T07:47:58.127848Z","iopub.status.idle":"2023-05-21T07:47:58.972039Z","shell.execute_reply":"2023-05-21T07:47:58.971100Z","shell.execute_reply.started":"2023-05-21T07:47:58.128150Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(0)\n","\n","SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n","TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n","EMB_SIZE = 512\n","NHEAD = 8\n","FFN_HID_DIM = 512\n","BATCH_SIZE = 128\n","NUM_ENCODER_LAYERS = 3\n","NUM_DECODER_LAYERS = 3\n","\n","transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n","                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n","\n","for p in transformer.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","transformer = transformer.to(DEVICE)\n","\n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T07:47:58.974366Z","iopub.status.busy":"2023-05-21T07:47:58.973360Z","iopub.status.idle":"2023-05-21T07:47:58.985300Z","shell.execute_reply":"2023-05-21T07:47:58.983619Z","shell.execute_reply.started":"2023-05-21T07:47:58.974329Z"},"trusted":true},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","from typing import List\n","\n","def sequential_transforms(*transforms: Callable) -> Callable:\n","    \"\"\"\n","    Helper function to club together sequential operations.\n","\n","    Args:\n","        *transforms: Sequence of functions to be applied sequentially.\n","\n","    Returns:\n","        func: A function that applies the sequence of transformations to the input.\n","    \"\"\"\n","    def func(txt_input: str):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","def tensor_transform(token_ids: List[int]) -> Tensor:\n","    \"\"\"\n","    Function to add BOS/EOS and create tensor for input sequence indices.\n","\n","    Args:\n","        token_ids (List[int]): List of token indices.\n","\n","    Returns:\n","        Tensor: Tensor containing the token indices with BOS/EOS tokens added.\n","    \"\"\"\n","    return torch.cat((torch.tensor([BOS_IDX]),\n","                      torch.tensor(token_ids),\n","                      torch.tensor([EOS_IDX])))\n","\n","text_transform = {}\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    text_transform[ln] = sequential_transforms(token_transform[ln],  # Tokenization\n","                                               vocab_transform[ln],  # Numericalization\n","                                               tensor_transform)  # Add BOS/EOS and create tensor\n","\n","def collate_fn(batch: List[Tuple[str, str]]) -> Tuple[Tensor, Tensor]:\n","    \"\"\"\n","    Function to collate data samples into batch tensors.\n","\n","    Args:\n","        batch (List[Tuple[str, str]]): List of tuples containing source and target sentences.\n","\n","    Returns:\n","        Tuple[Tensor, Tensor]: A tuple containing the source batch tensor and target batch tensor.\n","    \"\"\"\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n","\n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    return src_batch, tgt_batch"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T07:47:58.987933Z","iopub.status.busy":"2023-05-21T07:47:58.986975Z","iopub.status.idle":"2023-05-21T07:47:59.002660Z","shell.execute_reply":"2023-05-21T07:47:59.001677Z","shell.execute_reply.started":"2023-05-21T07:47:58.987894Z"},"trusted":true},"outputs":[],"source":["def train_epoch(model, optimizer) -> float:\n","    \"\"\"\n","    Perform one training epoch.\n","\n","    Args:\n","        model (Seq2SeqTransformer): The model to train.\n","        optimizer (torch.optim.Optimizer): The optimizer to use for training.\n","\n","    Returns:\n","        float: The average training loss for the epoch.\n","    \"\"\"\n","    model.train()\n","    losses = 0\n","    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in train_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(list(train_dataloader))\n","\n","\n","def evaluate(model) -> float:\n","    \"\"\"\n","    Perform evaluation on the validation set.\n","\n","    Args:\n","        model (Seq2SeqTransformer): The model to evaluate.\n","\n","    Returns:\n","        float: The average validation loss.\n","    \"\"\"\n","    model.eval()\n","    losses = 0\n","\n","    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n","\n","    for src, tgt in val_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        tgt_out = tgt[1:, :]\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        losses += loss.item()\n","\n","    return losses / len(list(val_dataloader))"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T07:47:59.006405Z","iopub.status.busy":"2023-05-21T07:47:59.006043Z","iopub.status.idle":"2023-05-21T08:02:35.962915Z","shell.execute_reply":"2023-05-21T08:02:35.961892Z","shell.execute_reply.started":"2023-05-21T07:47:59.006365Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1, Train loss: 5.344, Val loss: 4.104, Epoch time = 1214.843s\n","Epoch: 2, Train loss: 3.759, Val loss: 3.309, Epoch time = 1236.724s\n","Epoch: 3, Train loss: 3.159, Val loss: 2.891, Epoch time = 1225.386s\n","Epoch: 4, Train loss: 2.768, Val loss: 2.644, Epoch time = 1294.236s\n","Epoch: 5, Train loss: 2.479, Val loss: 2.443, Epoch time = 1239.040s\n","Epoch: 6, Train loss: 2.252, Val loss: 2.311, Epoch time = 1249.802s\n","Epoch: 7, Train loss: 2.062, Val loss: 2.197, Epoch time = 1187.805s\n","Epoch: 8, Train loss: 1.899, Val loss: 2.109, Epoch time = 1172.538s\n","Epoch: 9, Train loss: 1.756, Val loss: 2.062, Epoch time = 1195.688s\n","Epoch: 10, Train loss: 1.636, Val loss: 2.020, Epoch time = 1198.641s\n","Epoch: 11, Train loss: 1.524, Val loss: 1.972, Epoch time = 1167.054s\n","Epoch: 12, Train loss: 1.423, Val loss: 1.950, Epoch time = 1179.394s\n","Epoch: 13, Train loss: 1.333, Val loss: 1.942, Epoch time = 1236.072s\n","Epoch: 14, Train loss: 1.253, Val loss: 1.922, Epoch time = 1207.063s\n","Epoch: 15, Train loss: 1.177, Val loss: 1.908, Epoch time = 1172.697s\n","Epoch: 16, Train loss: 1.105, Val loss: 1.895, Epoch time = 1239.367s\n","Epoch: 17, Train loss: 1.037, Val loss: 1.906, Epoch time = 1174.294s\n","Epoch: 18, Train loss: 0.973, Val loss: 1.903, Epoch time = 1160.616s\n"]}],"source":["from timeit import default_timer as timer\n","NUM_EPOCHS = 18\n","\n","for epoch in range(1, NUM_EPOCHS+1):\n","    start_time = timer()\n","    train_loss = train_epoch(transformer, optimizer)\n","    end_time = timer()\n","    val_loss = evaluate(transformer)\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Questions Théoriques"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### In the positional encoding, why are we using a combination of sinus and cosinus?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["L'objectif de sin et de cos est de proposer une répartition cyclique comprise en permanence entre -1 et 1. Si l'on utilise une autre répartition, il serait fort possible que certaines valeurs deviennent excessivement grandes, ce qui n'aurait pas beaucoup d'intérêt. De plus, le fait d'alterner entre les sinus et les cosinus permet de pouvoir utiliser une transformation linéaire sur les vecteurs formés, en obtenant à la fin les mêmes fonctions mais avec un offset, ce qui permet une bonne modularité."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### In the Seq2SeqTransformer class, What is the parameter nhead for?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Dans Seq2SeqTransformer, le paramètre nhead permet de modifier le nombre de head qui seront utilisés par le module multi-head attention, qui permet de faire tourner un certain nombre de fois une fonction d'activation sur des inputs Q, K et V."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### What is the point of the generator?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Dans Seq2SeqTransformer, le générateur va permettre d'associer à chaque valeur la probabilité qu'il s'agisse de la meilleure valeur possible, en fonction des valeurs déjà détectées auparavant et de la valeur actuelle de l'input."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Describe the goal of the create_mask function. Why does it handle differently the source and target masks?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Decoding functions"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T09:28:30.794452Z","iopub.status.busy":"2023-05-21T09:28:30.794095Z","iopub.status.idle":"2023-05-21T09:28:30.882036Z","shell.execute_reply":"2023-05-21T09:28:30.881045Z","shell.execute_reply.started":"2023-05-21T09:28:30.794422Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" A group of people stand in front of an igloo \n"]}],"source":["def greedy_decode(model: Module, src: Tensor, src_mask: Tensor, max_len: int, start_symbol: int) -> Tensor:\n","    \"\"\"\n","    Generate an output sequence using a greedy decoding algorithm.\n","\n","    Args:\n","        model (Seq2SeqTransformer): The trained model.\n","        src (Tensor): The input source sequence.\n","        src_mask (Tensor): The source mask.\n","        max_len (int): The maximum length of the output sequence.\n","        start_symbol (int): The index of the start symbol.\n","\n","    Returns:\n","        Tensor: The generated output sequence.\n","    \"\"\"\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len-1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        _, next_word = torch.max(prob, dim=1)\n","        next_word = next_word.item()\n","\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","\n","def translate_greedy(model: torch.nn.Module, src_sentence: str) -> str:\n","    \"\"\"\n","    Translate an input sentence into the target language using greedy decoding.\n","\n","    Args:\n","        model (Seq2SeqTransformer): The trained model.\n","        src_sentence (str): The input source sentence.\n","\n","    Returns:\n","        str: The translated sentence in the target language.\n","    \"\"\"\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = greedy_decode(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n","\n","\n","print(translate_greedy(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T09:28:43.565615Z","iopub.status.busy":"2023-05-21T09:28:43.565253Z","iopub.status.idle":"2023-05-21T09:28:43.655069Z","shell.execute_reply":"2023-05-21T09:28:43.654087Z","shell.execute_reply.started":"2023-05-21T09:28:43.565586Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" A group of people standing in front of an igloo \n"]}],"source":["import torch.nn.functional as F\n","import numpy as np\n","\n","def top_k_decode(model: torch.nn.Module, src: torch.Tensor, src_mask: torch.Tensor, max_len: int, start_symbol: int, k: int, temp: float) -> torch.Tensor:    \n","    \"\"\"\n","    Generate an output sequence using top-k decoding algorithm.\n","\n","    Args:\n","        model (Seq2SeqTransformer): The trained model.\n","        src (Tensor): The input source sequence.\n","        src_mask (Tensor): The source mask.\n","        max_len (int): The maximum length of the output sequence.\n","        start_symbol (int): The index of the start symbol.\n","        k (int): The number of top-k candidates to consider.\n","        temp (float): The temperature parameter for controlling randomness.\n","\n","    Returns:\n","        Tensor: The generated output sequence.\n","    \"\"\"\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len - 1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        a, next_word = torch.topk(prob.flatten(), k)\n","        a = F.softmax(a / temp, dim=0)\n","        a = a.cpu().detach().numpy()\n","        next_word = next_word.cpu().numpy()\n","        next_word = np.random.choice(next_word, 1, p=(a / a.sum()))[0]\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","\n","def translate_top_k(model: torch.nn.Module, src_sentence: str, k: int, temp: float) -> str:\n","    \"\"\"\n","    Translate an input sentence into the target language using top-k decoding.\n","\n","    Args:\n","        model (Seq2SeqTransformer): The trained model.\n","        src_sentence (str): The input source sentence.\n","        k (int): The number of top-k candidates to consider.\n","        temp (float): The temperature parameter for controlling randomness.\n","\n","    Returns:\n","        str: The translated sentence in the target language.\n","    \"\"\"\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = top_k_decode(\n","        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, k=k, temp=temp).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n","\n","\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k=5, temp=0.7))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T09:29:41.394896Z","iopub.status.busy":"2023-05-21T09:29:41.394522Z","iopub.status.idle":"2023-05-21T09:29:41.498413Z","shell.execute_reply":"2023-05-21T09:29:41.497301Z","shell.execute_reply.started":"2023-05-21T09:29:41.394866Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" A crowd of People in an igloo area . \n"]}],"source":["def top_p_decode(model: torch.nn.Module, src: torch.Tensor, src_mask: torch.Tensor, max_len: int, start_symbol: int, p: float, temp: float) -> torch.Tensor:\n","    \"\"\"\n","    Generate an output sequence using top-p (nucleus) decoding algorithm.\n","\n","    Args:\n","        model (Seq2SeqTransformer): The trained model.\n","        src (Tensor): The input source sequence.\n","        src_mask (Tensor): The source mask.\n","        max_len (int): The maximum length of the output sequence.\n","        start_symbol (int): The index of the start symbol.\n","        p (float): The probability threshold for nucleus sampling.\n","        temp (float): The temperature parameter for controlling randomness.\n","\n","    Returns:\n","        Tensor: The generated output sequence.\n","    \"\"\"\n","    src = src.to(DEVICE)\n","    src_mask = src_mask.to(DEVICE)\n","\n","    memory = model.encode(src, src_mask)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n","    for i in range(max_len - 1):\n","        memory = memory.to(DEVICE)\n","        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n","                    .type(torch.bool)).to(DEVICE)\n","        out = model.decode(ys, memory, tgt_mask)\n","        out = out.transpose(0, 1)\n","        prob = model.generator(out[:, -1])\n","        prob = prob - prob.min()\n","        prob = prob / prob.sum()\n","        sorted_prob, sorted_indices = torch.sort(prob.flatten(), descending=True)\n","        cumulative_probs = torch.cumsum(sorted_prob, dim=0)\n","        truncate_indices = cumulative_probs > p\n","        truncate_indices[1:] = truncate_indices[:-1]\n","        truncate_indices[0] = False\n","        next_word_indices = sorted_indices[truncate_indices]\n","        next_word_probs = sorted_prob[truncate_indices] / sorted_prob[truncate_indices].sum()\n","\n","        next_word = np.random.choice(next_word_indices.cpu().numpy(), 1, p=next_word_probs.cpu().numpy())[0]\n","        ys = torch.cat([ys,\n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n","        if next_word == EOS_IDX:\n","            break\n","    return ys\n","\n","\n","def translate_top_p(model: torch.nn.Module, src_sentence: str, p: float, temp: float) -> str:\n","    \"\"\"\n","    Translate an input sentence into the target language using top-p (nucleus) decoding.\n","\n","    Args:\n","        model (Seq2SeqTransformer): The trained model.\n","        src_sentence (str): The input source sentence.\n","        p (float): The probability threshold for nucleus sampling.\n","        temp (float): The temperature parameter for controlling randomness.\n","\n","    Returns:\n","        str: The translated sentence in the target language.\n","    \"\"\"\n","    model.eval()\n","    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","    num_tokens = src.shape[0]\n","    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","    tgt_tokens = top_p_decode(model, src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, p=p, temp=temp).flatten()\n","    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n","\n","\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p=0.0006, temp=0.7))"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-05-21T09:45:07.824610Z","iopub.status.busy":"2023-05-21T09:45:07.824059Z","iopub.status.idle":"2023-05-21T09:45:10.348078Z","shell.execute_reply":"2023-05-21T09:45:10.346976Z","shell.execute_reply.started":"2023-05-21T09:45:07.824569Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" A group of people stand in front of an igloo \n","\n"," A group of people stand in front of an igloo \n"," A group of people in front of an igloo \n"," A group of people stand in front an auditorium . \n","\n"," A group of people in front of an envelope . \n"," A group of people stand in front of an abandoned facility . \n"," Group of people stand in front an igloo . \n","\n"," A group of people standing in front of an auditorium . \n"," A group of people standing in front of an igloo \n"," A group of people stand in front an igloo . \n","\n"," A group of people in front an auditorium . \n"," A group of people stand in front of an igloo . \n"," A group of people standing in front of an igloo . \n","\n"," A crowd of individuals stand in front of microphones . \n"," Group Group in midair standing in front an office setting . \n"," Group of people standing in front of an igloo area . \n","\n"," Group of People stand in midair . area \n"," A group is standing in an olive igloo . area \n"," Group Group of individuals standing in an office area \n","\n"," The crowd are gathered outside a igloo area . \n"," Group A bunch Group in a ATM machines machines him him . \" A lot\n"," A crowd of individuals in before one an issue \n","\n"," There stand a lot of persons outside an olive machines . \n"," The bunch is standing outside an elaborately indoors in an auditorium . \" him \n"," There is a lot full people stand before some sunglasses \n"]}],"source":["print(translate_greedy(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n","print()\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 5, temp = 0.7))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 5, temp = 0.7))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 5, temp = 0.7))\n","print()\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 5, temp = 0.9))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 5, temp = 0.9))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 5, temp = 0.9))\n","print()\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 10, temp = 0.7))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 10, temp = 0.7))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 10, temp = 0.7))\n","print()\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 10, temp = 0.9))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 10, temp = 0.9))\n","print(translate_top_k(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", k = 10, temp = 0.9))\n","print()\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.0006, temp = 0.7))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.0006, temp = 0.7))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.0006, temp = 0.7))\n","print()\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.0006, temp = 0.9))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.0006, temp = 0.9))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.0006, temp = 0.9))\n","print()\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.001, temp = 0.7))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.001, temp = 0.7))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.001, temp = 0.7))\n","print()\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.001, temp = 0.9))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.001, temp = 0.9))\n","print(translate_top_p(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\", p = 0.001, temp = 0.9))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["On peut noter que le modèle Greedy étant déterministe et se basant à chaque fois sur la probabilité la plus élevée, il y a une forte probabilité d'obtenir une bonne réponse. Les deux autres modèles, eux, ne sont pas déterministes et peuvent donner des résultats différents. On peut également noter que le modèle top_k reste relativement cohérent à mesure que l'on augmente la taille du k-sample, alors que le modèle top_p va très rapidement diverger. Le paramètre de température, lui, permet de rester plus ou moins dans le thème initial : plus il est faible (inférieur à 1), et plus les valeurs à haute probabilité auront une chance d'être sélectionnées pour la prédiction finale.\n","\n","De ce que l'on peut en voir, on peut donc en conclure qu'avec le dataset que l'on a, le modèle le plus efficace est celui qui est Greedy, mais le modèle de Top_k est également très efficace et permet d'obtenir plusieurs résultats, ce qui peut dans certains cas se révéler plus avantageux que le Greedy. Le modèle de top_p, lui, n'est pas très efficace dû à la taille importante du dataset qu'il a à prendre en compte ; l'idéal serait pour lui de moduler la valeur p en fonction de chaque mot."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Compute the BLEU score of the model"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/nanditraore/anaconda3/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n","  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"]}],"source":["import sacrebleu\n","from torchtext.data.metrics import bleu_score\n","from sacrebleu.metrics import BLEU\n","\n","def calculate_bleu(data: Dataset, model: Module, decoding_approach: str, k: int = None, temp: float = None, p: float = None) -> float:    \"\"\"\n","    Calculate the BLEU score for a given dataset using a specific decoding approach.\n","\n","    Args:\n","        data (torchtext.data.Dataset): The dataset containing source and target sentences.\n","        model (torch.nn.Module): The trained model for translation.\n","        decoding_approach (str): The decoding approach to use. Possible values: \"greedy\", \"top_k\", \"top_p\".\n","        k (int, optional): The value of k for top-k decoding. Only used when decoding_approach is \"top_k\".\n","        temp (float, optional): The temperature parameter for decoding. Only used when decoding_approach is \"top_k\" or \"top_p\".\n","        p (float, optional): The value of p for top-p decoding. Only used when decoding_approach is \"top_p\".\n","\n","    Returns:\n","        bleu_score (float): The BLEU score of the predicted translations.\n","\n","    Raises:\n","        ValueError: If an invalid decoding_approach is provided.\n","\n","    \"\"\"\n","\n","    trgs = []\n","    pred_trgs = []\n","    \n","    for src, target in data:\n","\n","        if decoding_approach == \"greedy\":\n","            pred_trg = translate_greedy(model, src)\n","        elif decoding_approach == \"top_k\":\n","            pred_trg = translate_top_k(model, src, k=k, temp=temp)\n","        elif decoding_approach == \"top_p\":\n","            pred_trg = translate_top_p(model, src, p=p, temp=temp)\n","        else:\n","            raise ValueError(\"Invalid decoding approach\")\n","\n","        pred_trg = pred_trg[:-1] \n","        pred_trgs.append(pred_trg)\n","        trgs.append([target])\n","\n","        \n","    bleu = BLEU()\n","    return bleu.corpus_score(pred_trgs, trgs) # .corpus_bleu(pred_trgs, trgs)\n","\n","test_iter = Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n","\n","bleu_greedy = calculate_bleu(test_iter, transformer, \"greedy\")\n","bleu_top_k = calculate_bleu(test_iter, transformer, \"top_k\", k=5, temp=0.7)\n","bleu_top_p = calculate_bleu(test_iter, transformer, \"top_p\", p=0.0006, temp=0.7)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Greedy BLEU score =  BLEU = 60.43 100.0/80.0/44.4/37.5 (BP = 1.000 ratio = 1.000 hyp_len = 11 ref_len = 11)\n","Top-k BLEU score =  BLEU = 50.81 91.7/54.5/40.0/33.3 (BP = 1.000 ratio = 1.000 hyp_len = 12 ref_len = 12)\n","Top-p BLEU score =  BLEU = 33.52 90.9/50.0/22.2/12.5 (BP = 1.000 ratio = 1.000 hyp_len = 11 ref_len = 11)\n"]}],"source":["print('Greedy BLEU score = ', bleu_greedy)\n","print('Top-k BLEU score = ', bleu_top_k)\n","print('Top-p BLEU score = ', bleu_top_p)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["BLEU : c'est le score BLEU global pour le corpus, calculé comme une moyenne géométrique des précisions n-grammes, pondérée par une pénalisation pour les phrases trop courtes (BP). \n","\n","counts: Un tuple contenant le nombre de n-grammes correspondants pour chaque ordre de n-grammes (1-grammes, 2-grammes, etc.).\n","\n","totals: Un tuple contenant le nombre total de n-grammes pour chaque ordre de n-grammes.\n","\n","precisions: Un tuple contenant les précisions pour chaque ordre de n-grammes.\n","\n","BP: c'est le facteur de pénalisation de longueur (Brevity Penalty). Si la longueur de la traduction est égale ou supérieure à la longueur de la référence, BP vaut 1, sinon il est inférieur à 1. Cela pénalise les traductions qui sont beaucoup plus courtes que les références.\n","\n","ratio: c'est le rapport entre la longueur de la traduction et la longueur de la référence. Un ratio de 1 signifie que la traduction et la référence ont la même longueur.\n","\n","hyp_len: c'est la longueur totale des traductions (hypothèses) dans le corpus.\n","\n","ref_len: c'est la longueur totale des références dans le corpus."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Bonus 1: Hyperparameters search "]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best score: 42.7287006396234\n","Best parameters: {'k': 11, 'p': 0.0012, 'temp': 0.9}\n","Best approach: top_k\n"]}],"source":["from sklearn.model_selection import ParameterGrid\n","\n","param_grid = {\n","    'temp': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n","    'k': [3, 5, 7, 9, 11],\n","    'p': [0.0003, 0.0006, 0.0009, 0.0012, 0.0015]\n","}\n","grid = ParameterGrid(param_grid)\n","\n","best_score = 0\n","best_params = None\n","\n","# We will use 20% of the test data for hyperparameter tuning\n","# Convert the test_iter to a list\n","test_iter = list(Multi30k(split='test', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE)))\n","validation_data, _ = torch.utils.data.random_split(test_iter, [int(len(test_iter)*0.2), len(test_iter) - int(len(test_iter)*0.2)])\n","\n","for params in grid:\n","    temp = params['temp']\n","    k = params['k']\n","    p = params['p']\n","    \n","    score_top_k= calculate_bleu(validation_data, transformer, \"top_k\", k=k, temp=temp).score\n","    score_top_p = calculate_bleu(validation_data, transformer, \"top_p\", p=p, temp=temp).score\n","    if score_top_k > best_score:\n","        best_score = score_top_k\n","        best_params = params\n","        best_approach = \"top_k\"\n","    if score_top_p > best_score:\n","        best_score = score_top_p\n","        best_params = params\n","        best_approach = \"top_p\"\n","        \n","print(f'Best score: {best_score}')\n","print(f'Best parameters: {best_params}')\n","print(f'Best approach: {best_approach}')\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["def translate_batch_greedy(model: torch.nn.Module, src_sentences: List[str]) -> List[str]:\n","    \"\"\"\n","    Translate a batch of source sentences using the greedy decoding approach.\n","\n","    Args:\n","        model (torch.nn.Module): The trained model for translation.\n","        src_sentences (List[str]): A list of source sentences to be translated.\n","\n","    Returns:\n","        translated_sentences (List[str]): A list of translated sentences.\n","\n","    \"\"\"\n","\n","    model.eval()\n","    translated_sentences = []\n","    for src_sentence in src_sentences:\n","        src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","        num_tokens = src.shape[0]\n","        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","        tgt_tokens = greedy_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n","        translated_sentence = \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n","        translated_sentences.append(translated_sentence)\n","    return translated_sentences\n","\n","def translate_batch_top_k(model: torch.nn.Module, src_sentences: List[str], k: int, temp: float) -> List[str]:\n","    \"\"\"\n","    Translate a batch of source sentences using the top-k decoding approach.\n","\n","    Args:\n","        model (torch.nn.Module): The trained model for translation.\n","        src_sentences (List[str]): A list of source sentences to be translated.\n","        k (int): The value of k for top-k decoding.\n","        temp (float): The temperature parameter for decoding.\n","\n","    Returns:\n","        translated_sentences (List[str]): A list of translated sentences.\n","\n","    \"\"\"\n","    model.eval()\n","    translated_sentences = []\n","    for src_sentence in src_sentences:\n","        src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","        num_tokens = src.shape[0]\n","        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","        tgt_tokens = top_k_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, k = k, temp = temp).flatten()\n","        translated_sentence = \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n","        translated_sentences.append(translated_sentence)\n","    return translated_sentences\n","\n","def translate_batch_top_p(model: torch.nn.Module, src_sentences: List[str], p: float, temp: float) -> List[str]:\n","    \"\"\"\n","    Translate a batch of source sentences using the top-p decoding approach.\n","\n","    Args:\n","        model (torch.nn.Module): The trained model for translation.\n","        src_sentences (List[str]): A list of source sentences to be translated.\n","        p (float): The value of p for top-p decoding.\n","        temp (float): The temperature parameter for decoding.\n","\n","    Returns:\n","        translated_sentences (List[str]): A list of translated sentences.\n","\n","    \"\"\"\n","    model.eval()\n","    translated_sentences = []\n","    for src_sentence in src_sentences:\n","        src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n","        num_tokens = src.shape[0]\n","        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n","        tgt_tokens = top_p_decode(model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX, p = p, temp = temp).flatten()\n","        translated_sentence = \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n","        translated_sentences.append(translated_sentence)\n","    return translated_sentences\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
